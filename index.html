<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Human Pose and Analysis</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      margin: 0;
      background-color: #f4f4f4;
      flex-direction: column;
    }
    #webcam {
      border: 2px solid black;
      width: 80%;
      max-width: 640px;
      margin-bottom: 20px;
    }
    #canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      pointer-events: none;
    }
    h1 {
      font-size: 24px;
    }
    .detected-info {
      font-size: 18px;
      margin-top: 20px;
      padding: 10px;
      background-color: rgba(0, 0, 0, 0.5);
      color: white;
      border-radius: 5px;
      text-align: center;
    }
  </style>
</head>
<body>
  <h1>Human Pose and Analysis</h1>
  <video id="webcam" autoplay></video>
  <canvas id="canvas"></canvas>
  <div class="detected-info" id="detectedInfo">Loading model...</div>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>
  <script src="https://cdn.jsdelivr.net/npm/face-api.js"></script>

  <script>
    const video = document.getElementById('webcam');
    const canvas = document.getElementById('canvas');
    const context = canvas.getContext('2d');
    const detectedInfo = document.getElementById('detectedInfo');
    let poseNetModel, faceApiLoaded = false;

    async function setupWebcam() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { width: 640, height: 480 },
        });
        video.srcObject = stream;

        video.onloadedmetadata = () => {
          console.log("Webcam loaded successfully");
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          detectPoseAndFace();
        };
      } catch (err) {
        console.error("Error accessing webcam: ", err);
        detectedInfo.textContent = "Unable to access the camera.";
      }
    }

    async function loadPoseNet() {
      try {
        poseNetModel = await posenet.load({
          architecture: 'MobileNetV1',
          outputStride: 16,
          inputResolution: { width: 640, height: 480 },
          multiplier: 0.5,
        });
        detectedInfo.textContent = 'PoseNet model loaded. Waiting for pose detection...';
      } catch (err) {
        console.error("Error loading PoseNet: ", err);
        detectedInfo.textContent = 'Error loading PoseNet model.';
      }
    }

    async function loadFaceApi() {
      try {
        await faceapi.nets.ssdMobilenetv1.loadFromUri('./models');
        await faceapi.nets.ageGenderNet.loadFromUri('./models');
        faceApiLoaded = true;
        detectedInfo.textContent = 'FaceAPI model loaded. Waiting for face detection...';
      } catch (err) {
        console.error("Error loading FaceAPI: ", err);
        detectedInfo.textContent = 'Error loading FaceAPI model.';
      }
    }

    async function detectPoseAndFace() {
      if (!poseNetModel || !faceApiLoaded) return;

      context.drawImage(video, 0, 0, canvas.width, canvas.height);

      const pose = await poseNetModel.estimateSinglePose(video, { flipHorizontal: false });
      drawKeypoints(pose.keypoints);

      const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withAgeAndGender();
      detections.forEach((detection) => {
        const { age, gender } = detection;
        context.beginPath();
        context.rect(detection.detection.box.x, detection.detection.box.y, detection.detection.box.width, detection.detection.box.height);
        context.lineWidth = 2;
        context.strokeStyle = 'green';
        context.stroke();
        context.fillStyle = 'green';
        context.fillText(`${gender}, ${Math.round(age)} years`, detection.detection.box.x, detection.detection.box.y - 5);
      });

      if (detections.length > 0) {
        const { age, gender } = detections[0];
        detectedInfo.textContent = `Detected Age: ${Math.round(age)} years, Gender: ${gender}`;
      } else {
        detectedInfo.textContent = 'No face detected.';
      }

      requestAnimationFrame(detectPoseAndFace);
    }

    function drawKeypoints(keypoints) {
      keypoints.forEach((keypoint) => {
        if (keypoint.score > 0.5) {
          context.beginPath();
          context.arc(keypoint.position.x, keypoint.position.y, 5, 0, 2 * Math.PI);
          context.fillStyle = 'red';
          context.fill();
        }
      });
    }

    async function init() {
      await loadPoseNet();
      await loadFaceApi();
      await setupWebcam();
    }

    init();
  </script>
</body>
</html>
